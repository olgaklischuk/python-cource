{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%conda install numpy \n",
    "%conda install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "df  = pd.read_csv(\"creditcard.csv\")[:80_000]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Time', 'Amount', 'Class']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Shapes of X={X.shape} y={y.shape}, #Fraud Cases={y.sum()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mod = LogisticRegression(class_weight={0: 1, 1: 2}, max_iter=1000)\n",
    "mod.fit(X, y).predict(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_recall_precision(y_true, y_pred):\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    return min(recall, precision)\n",
    "\n",
    "make_scorer(min_recall_precision, greater_is_better=False)\n",
    "# ?make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
    "\n",
    "def min_recall_precision(est, X, y_true, sample_weight=None):\n",
    "    y_pred = est.predict(X)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    return min(recall, precision)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},\n",
    "    scoring={'precision': make_scorer(precision_score), \n",
    "             'recall': make_scorer(recall_score),\n",
    "             'min_both': min_recall_precision},\n",
    "    refit='min_both',\n",
    "    return_train_score=True,\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = make_scorer(min_recall_precision)\n",
    "# ??s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary for the test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "df_results = pd.DataFrame(grid.cv_results_)\n",
    "for score in ['mean_test_recall', 'mean_test_precision', 'mean_test_min_both']:\n",
    "    plt.plot([_[1] for _ in df_results['param_class_weight']], \n",
    "             df_results[score], \n",
    "             label=score)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the train metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "df_results = pd.DataFrame(grid.cv_results_)\n",
    "for score in ['mean_train_recall', 'mean_train_precision', 'mean_test_min_both']:\n",
    "    plt.scatter(x=[_[1] for _ in df_results['param_class_weight']], \n",
    "                y=df_results[score.replace('test', 'train')], \n",
    "                label=score)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Outlier Detection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.ensemble import IsolationForest\n",
    "mod = IsolationForest().fit(X)\n",
    "np.where(mod.predict(X) == -1, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now in a gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_precision(mod, X, y):\n",
    "    preds = mod.predict(X)\n",
    "    return precision_score(y, np.where(preds == -1, 1, 0))\n",
    "\n",
    "def outlier_recall(mod, X, y):\n",
    "    preds = mod.predict(X)\n",
    "    return recall_score(y, np.where(preds == -1, 1, 0))\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=IsolationForest(),\n",
    "    param_grid={'contamination': np.linspace(0.001, 0.02, 10)},\n",
    "    scoring={'precision': outlier_precision, \n",
    "             'recall': outlier_recall},\n",
    "    refit='precision',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y);\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "df_results = pd.DataFrame(grid.cv_results_)\n",
    "for score in ['mean_test_recall', 'mean_test_precision']:\n",
    "    plt.plot(df_results['param_contamination'], \n",
    "             df_results[score], \n",
    "             label=score)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid.cv_results_)\n",
    "plt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_recall'])\n",
    "plt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_precision']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_pre_rec(y, y_true):\n",
    "    return min(recall_score(y, y_true), precision_score(y, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_precision(mod, X, y):\n",
    "    preds = mod.predict(X)\n",
    "    return precision_score(y, np.where(preds == 1, 0, 1))\n",
    "\n",
    "def outlier_recall(mod, X, y):\n",
    "    preds = mod.predict(X)\n",
    "    return recall_score(y, np.where(preds == 1, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    estimator=LogisticRegression(class_weight=10),\n",
    "    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 40, t5)]},\n",
    "    scoring={'precision': make_scorer(precision_score), 'recall': make_scorer(recall_score), 'min_pre_rec': make_scorer(min_pre_rec)},\n",
    "    refit='precision',\n",
    "    cv = 10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid.cv_results_)\n",
    "plt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_recall'])\n",
    "plt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_precision'])\n",
    "plt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_min_pre_rec']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = make_scorer(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
