{"metadata":{"file_extension":".py","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%conda install numpy \n%conda install matplotlib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\ndf  = pd.read_csv(\"creditcard.csv\")[:80_000]\ndf.head()\ndf[df['Class']=='nan'].head()[]","metadata":{"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  res_values = method(rvalues)\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class]\nIndex: []\n\n[0 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows Ã— 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X = df.drop(columns=['Time', 'Amount', 'Class']).values","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"y = df['Class'].values","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"y.sum()","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"196.0"},"metadata":{}}]},{"cell_type":"code","source":"f\"Shapes of X={X.shape} y={y.shape}, #Fraud Cases={y.sum()}\"","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'Shapes of X=(80000, 28) y=(80000,), #Fraud Cases=196.0'"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmod = LogisticRegression(class_weight={0: 1, 1: 2}, max_iter=1000)\nmod.fit(X, y).predict(X).sum()","metadata":{"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"171.0"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ngrid = GridSearchCV(estimator = LogisticRegression(max_iter=1000),\n    param_grid = {'class_weight': [{0: 1, 1: v} for v in range(1, 4)]}, \n    cv = 4,\n    n_jobs = -1)\ngrid.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(grid.cv_results_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, make_scorer\n\ndef min_recall_precision(est, X, y_true, sample_weight=None):\n    y_pred = est.predict(X)\n    recall = recall_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    return min(recall, precision)\n\ngrid = GridSearchCV(\n    estimator=LogisticRegression(max_iter=1000),\n    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},\n    scoring={'precision': make_scorer(precision_score), \n             'recall': make_scorer(recall_score),\n             'min_both': min_recall_precision},\n    refit='min_both',\n    return_train_score=True,\n    cv=10,\n    n_jobs=-1\n)\ngrid.fit(X, y);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def min_recall_precision(y_true, y_pred):\n    recall = recall_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    return min(recall, precision)\n\nmake_scorer(min_recall_precision, greater_is_better=False)\n# ?make_scorer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# s = make_scorer(min_recall_precision)\n# ??s","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a summary for the test metrics.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\ndf_results = pd.DataFrame(grid.cv_results_)\nfor score in ['mean_test_recall', 'mean_test_precision', 'mean_test_min_both']:\n    plt.plot([_[1] for _ in df_results['param_class_weight']], \n             df_results[score], \n             label=score)\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here's the train metrics.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\ndf_results = pd.DataFrame(grid.cv_results_)\nfor score in ['mean_train_recall', 'mean_train_precision', 'mean_test_min_both']:\n    plt.scatter(x=[_[1] for _ in df_results['param_class_weight']], \n                y=df_results[score.replace('test', 'train')], \n                label=score)\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Outlier Detection Models","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom sklearn.ensemble import IsolationForest\nmod = IsolationForest().fit(X)\nnp.where(mod.predict(X) == -1, 1, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now in a gridsearch.","metadata":{}},{"cell_type":"code","source":"def outlier_precision(mod, X, y):\n    preds = mod.predict(X)\n    return precision_score(y, np.where(preds == -1, 1, 0))\n\ndef outlier_recall(mod, X, y):\n    preds = mod.predict(X)\n    return recall_score(y, np.where(preds == -1, 1, 0))\n\ngrid = GridSearchCV(\n    estimator=IsolationForest(),\n    param_grid={'contamination': np.linspace(0.001, 0.02, 10)},\n    scoring={'precision': outlier_precision, \n             'recall': outlier_recall},\n    refit='precision',\n    cv=5,\n    n_jobs=-1\n)\ngrid.fit(X, y);\n\nplt.figure(figsize=(12, 4))\ndf_results = pd.DataFrame(grid.cv_results_)\nfor score in ['mean_test_recall', 'mean_test_precision']:\n    plt.plot(df_results['param_contamination'], \n             df_results[score], \n             label=score)\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br><br><br><br><br><br><br><br><br><br><br>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(grid.cv_results_)\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_recall'])\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_precision']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def min_pre_rec(y, y_true):\n    return min(recall_score(y, y_true), precision_score(y, y_true))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outlier_precision(mod, X, y):\n    preds = mod.predict(X)\n    return precision_score(y, np.where(preds == 1, 0, 1))\n\ndef outlier_recall(mod, X, y):\n    preds = mod.predict(X)\n    return recall_score(y, np.where(preds == 1, 0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(\n    estimator=LogisticRegression(class_weight=10),\n    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 40, t5)]},\n    scoring={'precision': make_scorer(precision_score), 'recall': make_scorer(recall_score), 'min_pre_rec': make_scorer(min_pre_rec)},\n    refit='precision',\n    cv = 10,\n    n_jobs=-1\n)\ngrid.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(grid.cv_results_)\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_recall'])\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_precision'])\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_min_pre_rec']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = make_scorer(recall_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \n\nnp.eye(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1 + 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}