{"metadata":{"file_extension":".py","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# $ python -m pip install numpy\n# $ python -m pip install pandas\n# $ python -m pip install matplotlib\n# $ python -m pip install scikit-learn\n# $ python -m pip install keras\n# $ python -m pip install tensorflow","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\nbinder=0\nif bool(input(binder))== True:\n    df = pd.read_csv(\"creditcard.csv\")\nelse:\n    df  = pd.read_csv(\"C:/Users/Lenovo/OneDrive/Data Science project/Python.Data.Science/scikit-learn/scikit-metrics/creditcard.csv\")[:80_000]\ndf.head(3)","metadata":{"scrolled":true,"tags":[],"trusted":true},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdin","text":"0 True\n"}]},{"cell_type":"code","source":"X = df.drop(columns=['Time', 'Amount', 'Class']).values\ny = df['Class'].values\nf\"Shapes of X={X.shape} y={y.shape}, #Fraud Cases={y.sum()}\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmod = LogisticRegression(class_weight={0: 1, 1: 2}, max_iter=1000)\nmod.fit(X, y).predict(X).sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def min_recall_precision(y_true, y_pred):\n    recall = recall_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    return min(recall, precision)\n\nmake_scorer(min_recall_precision, greater_is_better=False)\n# ?make_scorer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_score, recall_score, make_scorer\n\ndef min_recall_precision(est, X, y_true, sample_weight=None):\n    y_pred = est.predict(X)\n    recall = recall_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    return min(recall, precision)\n\ngrid = GridSearchCV(\n    estimator=LogisticRegression(max_iter=1000),\n    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},\n    scoring={'precision': make_scorer(precision_score), \n             'recall': make_scorer(recall_score),\n             'min_both': min_recall_precision},\n    refit='min_both',\n    return_train_score=True,\n    cv=10,\n    n_jobs=-1\n)\ngrid.fit(X, y);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# s = make_scorer(min_recall_precision)\n# ??s","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a summary for the test metrics.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\ndf_results = pd.DataFrame(grid.cv_results_)\nfor score in ['mean_test_recall', 'mean_test_precision', 'mean_test_min_both']:\n    plt.plot([_[1] for _ in df_results['param_class_weight']], \n             df_results[score], \n             label=score)\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And here's the train metrics.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\ndf_results = pd.DataFrame(grid.cv_results_)\nfor score in ['mean_train_recall', 'mean_train_precision', 'mean_test_min_both']:\n    plt.scatter(x=[_[1] for _ in df_results['param_class_weight']], \n                y=df_results[score.replace('test', 'train')], \n                label=score)\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Outlier Detection Models","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom sklearn.ensemble import IsolationForest\nmod = IsolationForest().fit(X)\nnp.where(mod.predict(X) == -1, 1, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now in a gridsearch.","metadata":{}},{"cell_type":"code","source":"def outlier_precision(mod, X, y):\n    preds = mod.predict(X)\n    return precision_score(y, np.where(preds == -1, 1, 0))\n\ndef outlier_recall(mod, X, y):\n    preds = mod.predict(X)\n    return recall_score(y, np.where(preds == -1, 1, 0))\n\ngrid = GridSearchCV(\n    estimator=IsolationForest(),\n    param_grid={'contamination': np.linspace(0.001, 0.02, 10)},\n    scoring={'precision': outlier_precision, \n             'recall': outlier_recall},\n    refit='precision',\n    cv=5,\n    n_jobs=-1\n)\ngrid.fit(X, y);\n\nplt.figure(figsize=(12, 4))\ndf_results = pd.DataFrame(grid.cv_results_)\nfor score in ['mean_test_recall', 'mean_test_precision']:\n    plt.plot(df_results['param_contamination'], \n             df_results[score], \n             label=score)\nplt.legend();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br><br><br><br><br><br><br><br><br><br><br><br>","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(grid.cv_results_)\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_recall'])\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_precision']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def min_pre_rec(y, y_true):\n    return min(recall_score(y, y_true), precision_score(y, y_true))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def outlier_precision(mod, X, y):\n    preds = mod.predict(X)\n    return precision_score(y, np.where(preds == 1, 0, 1))\n\ndef outlier_recall(mod, X, y):\n    preds = mod.predict(X)\n    return recall_score(y, np.where(preds == 1, 0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(\n    estimator=LogisticRegression(class_weight=10),\n    param_grid={'class_weight': [{0: 1, 1: v} for v in np.linspace(1, 40, t5)]},\n    scoring={'precision': make_scorer(precision_score), 'recall': make_scorer(recall_score), 'min_pre_rec': make_scorer(min_pre_rec)},\n    refit='precision',\n    cv = 10,\n    n_jobs=-1\n)\ngrid.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(grid.cv_results_)\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_recall'])\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_precision'])\nplt.plot([_[1] for _ in df['param_class_weight']], df['mean_test_min_pre_rec']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = make_scorer(recall_score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \n\nnp.eye(4)","metadata":{},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])"},"metadata":{}}]}]}